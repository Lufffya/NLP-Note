## Embeddings from Language Model（ELMO）2018

- #### 概念

  - ELMO 是一个预训练的语言模型

  - ELMO 使用 双向的 LSTM 的网络架构

  

- #### 预训练任务

  - 根据前后上下文来预测句子里面的其他单词

  - 正向的 LSTM 根据单词的上文，预测单词的下文；反向的 LSTM 根据单词的下文，预测单词的上文

    

- #### 优点

  - 相对于 Word2vec 来说，ELMO 产生的词向量能够考虑上下文的关系，而不是固定的词只有固定的向量





## Attention is all your need（Transformer）2017

- #### 概念

  - Attention 机制|Attention 公式|Encoder 和 Decoder的区别




## Bidirectional Encoder Representations from Transformers（BERT）2018

- #### 概念

  - 



## Generative Pre-Training（GPT）2018

- #### 概念

  - 

